# -*- coding: utf-8 -*-
"""Stage.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1edrNgNb-6Q8mYhBBpQB2miYOcvBC8ECv

'''
Script de stage - Exploration & Analyse
Nettoyé et structuré en 4 grandes étapes :
1. Collecte & Préparation
2. Exploration & Analyse
3. Modélisation prédictive
4. Restitution (sauvegarde finale)
'''
"""

import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# --- 1. Chargement des CSV ---
df_adzuna = pd.read_csv("adzuna_offres_brutes.csv")
df_formations = pd.read_csv("openclassrooms_formations_final.csv")
df_remotive = pd.read_csv("remotive_jobs_clean.csv")

# --- 1.b Nettoyage du CSV des étudiants ---
df_etudiants = pd.read_csv("etudiants_interesses_web4jobs.csv", header=None)
first_row = df_etudiants.iloc[0].tolist()

if not any("Unnamed" in str(c) for c in first_row):
    df_etudiants.columns = first_row
    df_etudiants = df_etudiants.drop(0).reset_index(drop=True)

df_etudiants.columns = df_etudiants.columns.map(str)
df_etudiants = df_etudiants.loc[:, ~df_etudiants.columns.str.contains('^Unnamed')]

if df_etudiants.columns[0] != 'titre':
    df_etudiants.rename(columns={df_etudiants.columns[0]: 'titre'}, inplace=True)

# --- 2. Préparation NLTK ---
nltk.download('punkt')
nltk.download("punkt_tab")
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

def extract_keywords(text):
    text = str(text).lower()
    text = re.sub(r'[^a-z\s]', '', text)
    words = word_tokenize(text)
    return [w for w in words if w not in stop_words and len(w) > 2]

# --- 3. Extraction mots-clés ---
df_remotive['keywords'] = df_remotive['title'].apply(extract_keywords)

if 'title' in df_adzuna.columns:
    df_adzuna['keywords'] = df_adzuna['title'].apply(extract_keywords)
elif 'skills' in df_adzuna.columns:
    df_adzuna['keywords'] = df_adzuna['skills'].apply(lambda x: str(x).split(','))
else:
    df_adzuna['keywords'] = [[] for _ in range(len(df_adzuna))]

# --- 4. Mapping mots-clés -> formations ---
formations_list = df_formations.iloc[:, 0].astype(str).tolist()

def map_to_formations(keywords, formations_list):
    mapped = []
    for form in formations_list:
        for kw in keywords:
            if kw in form.lower():
                mapped.append(form)
                break
    return mapped if mapped else ['Autres']

df_remotive['formations_associees'] = df_remotive['keywords'].apply(lambda x: map_to_formations(x, formations_list))
df_adzuna['formations_associees'] = df_adzuna['keywords'].apply(lambda x: map_to_formations(x, formations_list))

# --- 5. Fusion marché ---
df_market = pd.concat([
    df_remotive[['keywords', 'formations_associees']],
    df_adzuna[['keywords', 'formations_associees']]
], ignore_index=True)

df_market_exploded = df_market.explode('formations_associees')
market_demand = df_market_exploded['formations_associees'].value_counts().reset_index()
market_demand.columns = ['formation', 'demand_offres']

# --- 6. Fusion formations ---
col_form = df_formations.columns[0]
df_final = pd.merge(df_formations, market_demand, left_on=col_form, right_on='formation', how='left')
df_final['demand_offres'] = df_final['demand_offres'].fillna(0).astype(int)

# --- 7. Fusion étudiants ---
col_etud = df_etudiants.columns[0]
df_final = pd.merge(df_final, df_etudiants, left_on=col_form, right_on=col_etud, how='left')

# Convertir la colonne "Étudiants Intéressés - Web4Jobs" en numérique (si existe)
if "Étudiants Intéressés - Web4Jobs" in df_final.columns:
    df_final["Étudiants Intéressés - Web4Jobs"] = pd.to_numeric(df_final["Étudiants Intéressés - Web4Jobs"], errors='coerce').fillna(0)

# --- 8. Calcul ratio demande/étudiants ---
if "Étudiants Intéressés - Web4Jobs" in df_final.columns:
    df_final['ratio_demande_etudiants'] = df_final.apply(
        lambda x: x['demand_offres'] / x["Étudiants Intéressés - Web4Jobs"] if x["Étudiants Intéressés - Web4Jobs"] > 0 else x['demand_offres'],
        axis=1
    )
else:
    df_final['ratio_demande_etudiants'] = df_final['demand_offres']

# --- 9. Tri par popularité ---
df_final_sorted = df_final.sort_values(by='demand_offres', ascending=False)

# --- 10. Top 10 des formations ---
top10_formations = df_final_sorted.head(10)
top10_formations.to_csv("top10_formations.csv", index=False, encoding='utf-8')

# --- 11. Sauvegarde du DataFrame complet ---
df_final_sorted.to_csv("df_final_clean.csv", index=False, encoding='utf-8')

print("✅ DataFrame final sauvegardé dans 'df_final_clean.csv'")
print("✅ Top 10 des formations sauvegardé dans 'top10_formations.csv'")
print(top10_formations[['titre', 'demand_offres', 'ratio_demande_etudiants']])
df = pd.read_csv("df_final_clean.csv")  # Recharger le fichier
df_cleaned = df.drop(columns=['1', '2', '3', '4', '5', '6', '7', '8'])
df_cleaned.to_csv("df_final_clean_no_empty.csv", index=False)

# --- Étape 2 : Exploration & Analyse ---



# Charger le fichier nettoyé
df = pd.read_csv("df_final_clean_no_empty.csv")

# --- 1. Top 10 des formations les plus demandées ---
plt.figure(figsize=(12,6))
df.sort_values(by="demand_offres", ascending=False).head(10)\
    .plot(x="titre", y="demand_offres", kind="barh", color="steelblue", legend=False)
plt.title("Top 10 des formations les plus demandées")
plt.xlabel("Nombre d'offres")
plt.ylabel("Formation")
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

# --- 2. Distribution des offres ---
plt.figure(figsize=(10,5))
sns.histplot(df['demand_offres'], bins=30, kde=True, color="green")
plt.title("Distribution de la demande (offres)")
plt.xlabel("Nombre d'offres")
plt.ylabel("Nombre de formations")
plt.tight_layout()
plt.show()

# --- 3. Ratio demande/étudiants ---
plt.figure(figsize=(10,5))
sns.histplot(df['ratio_demande_etudiants'], bins=30, kde=True, color="orange")
plt.title("Distribution du ratio Demande / Étudiants")
plt.xlabel("Ratio")
plt.ylabel("Nombre de formations")
plt.tight_layout()
plt.show()

# --- 4. Durée moyenne par catégorie ---
if "categorie" in df.columns:
    plt.figure(figsize=(12,6))
    df.groupby('categorie')['duree_heures'].mean().sort_values()\
        .plot(kind="barh", color="purple")
    plt.title("Durée moyenne des formations par catégorie")
    plt.xlabel("Durée (heures)")
    plt.tight_layout()
    plt.show()

# --- 5. Formations certifiantes vs non certifiantes ---
if "certification" in df.columns:
    plt.figure(figsize=(6,4))
    df['certification'].value_counts().plot(kind="bar", color="teal")
    plt.title("Formations certifiantes vs non certifiantes")
    plt.xlabel("Certification")
    plt.ylabel("Nombre de formations")
    plt.tight_layout()
    plt.show()

# --- 6. Heatmap de corrélation ---
plt.figure(figsize=(10,6))
sns.heatmap(df.select_dtypes(include="number").corr(), annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Matrice de corrélation entre variables numériques")
plt.tight_layout()
plt.show()



# --- 8. Boxplot : Ratio par catégorie ---
if "categorie" in df.columns:
    plt.figure(figsize=(12,6))
    sns.boxplot(x="categorie", y="ratio_demande_etudiants", data=df)
    plt.title("Répartition du ratio demande/étudiants par catégorie")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns  # tu utilises déjà seaborn

# 1) Préparer un df propre pour le plot
plot_df = df[['duree_heures', 'demand_offres', 'certification']].copy()
plot_df = plot_df.dropna()
plot_df = plot_df[plot_df['duree_heures'] > 0]

# Si certification est 0/1, rendre la légende plus claire
if plot_df['certification'].dtype != 'O':
    plot_df['certification'] = plot_df['certification'].map({1: 'Certifiante', 0: 'Non certifiante'}).fillna('Non renseigné')

# 2) Limiter l’influence des valeurs extrêmes (option A - bornes par quantiles)
xmax = plot_df['duree_heures'].quantile(0.98)
ymax = plot_df['demand_offres'].quantile(0.98)

plt.figure(figsize=(10, 6))
ax = sns.scatterplot(
    data=plot_df,
    x='duree_heures', y='demand_offres',
    hue='certification',
    s=40, alpha=0.6, edgecolor='none'
)

# Déplacer la légende hors du graphique
ax.legend(title='Certification', bbox_to_anchor=(1.02, 1), loc='upper left', frameon=False)

# Appliquer des bornes "raisonnables"
ax.set_xlim(0, xmax)
ax.set_ylim(0, ymax)

plt.title("Relation entre durée des formations et demande d'offres")
plt.xlabel("Durée (heures)")
plt.ylabel("Nombre d'offres")
plt.grid(True, linestyle='--', alpha=0.3)

# Laisser de la place pour la légende à droite
plt.tight_layout(rect=[0, 0, 0.82, 1])
plt.show()

import pandas as pd

df_formations = pd.read_csv("df_final_clean_no_empty.csv")
df_offres_ds = pd.read_csv("offres_data_scientist.csv")
df_organismes = pd.read_csv("organismes_numeriques_certifies.csv")
df_remotive = pd.read_csv("remotive_jobs_clean.csv")
df_stackoverflow = pd.read_csv("stackoverflow_trends.csv")
df_survey = pd.read_csv("survey_results_public.csv")
df_schema = pd.read_csv("survey_results_schema.csv")
df_google = pd.read_csv("tendances_google_france.csv")


datasets = {
    "formations": df_formations,
    "offres_ds": df_offres_ds,
    "organismes": df_organismes,
    "remotive": df_remotive,
    "stackoverflow": df_stackoverflow,
    "survey": df_survey,
    "schema": df_schema,
    "google": df_google
}

for name, df in datasets.items():
    print(f"\n--- {name.upper()} ---")
    print("Shape :", df.shape)
    print("Colonnes :", df.columns.tolist()[:10])  # affiche seulement les 10 premières colonnes
    print(df.head(2))

import matplotlib.pyplot as plt
import seaborn as sns

# Regrouper par thématique (categorie)
stats_thematiques = df_formations.groupby("categorie").agg({
    "demand_offres": "sum",
    "ratio_demande_etudiants": "mean",
    "duree_heures": "mean"
}).reset_index()

print(stats_thematiques)

# Visualisation : demande_offres par thématique
plt.figure(figsize=(10,6))
sns.barplot(data=stats_thematiques, x="categorie", y="demand_offres", palette="viridis")
plt.title("Demande d'offres par thématique digitale")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

#a) Évolution temporelle d’une techno
df_google['date'] = pd.to_datetime(df_google['date'])

plt.figure(figsize=(12,6))
for col in df_google.drop(columns="date").columns:
    plt.plot(df_google['date'], df_google[col], label=col)

plt.legend()
plt.title("Évolution de l'intérêt Google Trends par thématique digitale")
plt.xlabel("Date")
plt.ylabel("Popularité (Google Trends)")
plt.show()

#b) Identifier les thématiques en croissance / perte de vitesse
# Découper la période en deux moitiés
mid = len(df_google) // 2
first_half = df_google.iloc[:mid].drop(columns="date").mean()
second_half = df_google.iloc[mid:].drop(columns="date").mean()

# Croissance = différence entre fin et début
growth = (second_half - first_half).sort_values(ascending=False)

print("🚀 Thématiques en croissance :")
print(growth.head())

print("\n📉 Thématiques en perte de vitesse :")
print(growth.tail())

# Exemple : comparer Data Science (formations) avec Data Science (Google)
if "Data Science" in df_google.columns and "Data" in df_formations['categorie'].unique():
    google_trend_ds = df_google[['date','Data Science']]
    demandes_ds = df_formations[df_formations['categorie']=="Data"]["demand_offres"].sum()

    print("Demande totale formations Data Science :", demandes_ds)
    plt.figure(figsize=(10,5))
    plt.plot(google_trend_ds['date'], google_trend_ds['Data Science'])
    plt.title("Popularité Data Science (Google Trends) vs Inscriptions")
    plt.show()

# --- Étape 3 : Modélisation prédictive améliorée ---
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score

# 🔹 XGBoost
from xgboost import XGBRegressor

# ============================
# 1. Préparation du dataset
# ============================
df_final['duree_heures'] = pd.to_numeric(df_final['duree_heures'], errors='coerce').fillna(0)
df_final['certification'] = df_final['certification'].map({'oui': 1, 'non': 0}).fillna(0)
df_final = df_final[df_final['demand_offres'].notnull()]  # garder lignes avec la cible

# Variables explicatives
features = ['duree_heures', 'certification', 'categorie', 'langue', 'titre_simplifie']
X = df_final[features]
y = df_final['demand_offres']

# Colonnes par type
numeric_features = ['duree_heures', 'certification']
categorical_features = ['categorie', 'langue']
text_features = 'titre_simplifie'

# ============================
# 2. Préprocesseurs
# ============================
numeric_transformer = StandardScaler()
categorical_transformer = OneHotEncoder(handle_unknown="ignore")
text_transformer = TfidfVectorizer(max_features=100)

# ColumnTransformer combine tout
preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features),
        ("cat", categorical_transformer, categorical_features),
        ("text", text_transformer, text_features)
    ]
)

# ============================
# 3. Split Train/Test
# ============================
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ============================
# 4. Modèles
# ============================
models = {
    "Linear Regression": LinearRegression(),
    "Ridge Regression": Ridge(alpha=1.0),
    "Lasso Regression": Lasso(alpha=0.001, max_iter=5000),
    "Random Forest": RandomForestRegressor(random_state=42, n_estimators=200),
    "Gradient Boosting": GradientBoostingRegressor(random_state=42, n_estimators=200),
    "XGBoost": XGBRegressor(
        n_estimators=300, learning_rate=0.1, max_depth=6, subsample=0.8, colsample_bytree=0.8, random_state=42
    )
}

# ============================
# 5. Entraînement et évaluation
# ============================
results = {}
for name, model in models.items():
    pipe = Pipeline(steps=[("preprocessor", preprocessor),
                           ("model", model)])
    pipe.fit(X_train, y_train)
    y_pred = pipe.predict(X_test)

    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)
    results[name] = {"RMSE": rmse, "R²": r2}
    print(f"{name} → RMSE: {rmse:.2f}, R²: {r2:.2f}")

# ============================
# 6. Importance des variables
# ============================

# --- Random Forest ---
rf_pipe = Pipeline(steps=[("preprocessor", preprocessor),
                          ("model", RandomForestRegressor(random_state=42, n_estimators=200))])
rf_pipe.fit(X_train, y_train)
rf_model = rf_pipe.named_steps["model"]

# Récupérer noms des features
ohe_features = rf_pipe.named_steps["preprocessor"].transformers_[1][1].get_feature_names_out(categorical_features)
tfidf_features = rf_pipe.named_steps["preprocessor"].transformers_[2][1].get_feature_names_out()
feature_names = numeric_features + list(ohe_features) + list(tfidf_features)

# Importances
importances = rf_model.feature_importances_
feat_imp = pd.Series(importances, index=feature_names).sort_values(ascending=True).tail(15)

plt.figure(figsize=(8,6))
feat_imp.plot(kind="barh")
plt.title("Importance des variables (Random Forest)")
plt.show()

# --- XGBoost ---
xgb_model = XGBRegressor(n_estimators=300, learning_rate=0.1, max_depth=6, subsample=0.8, colsample_bytree=0.8, random_state=42)
xgb_pipe = Pipeline(steps=[("preprocessor", preprocessor),
                           ("model", xgb_model)])
xgb_pipe.fit(X_train, y_train)

xgb_importances = xgb_pipe.named_steps["model"].feature_importances_
feat_imp_xgb = pd.Series(xgb_importances, index=feature_names).sort_values(ascending=True).tail(15)

plt.figure(figsize=(8,6))
feat_imp_xgb.plot(kind="barh", color="orange")
plt.title("Importance des variables (XGBoost)")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# 📌 Résultats déjà obtenus
results = {
    "Linear Regression": {"RMSE": 161.80, "R²": 0.69},
    "Ridge Regression": {"RMSE": 186.98, "R²": 0.59},
    "Lasso Regression": {"RMSE": 194.05, "R²": 0.56},
    "Random Forest": {"RMSE": 198.20, "R²": 0.54},
    "Gradient Boosting": {"RMSE": 167.35, "R²": 0.67},
    "XGBoost": {"RMSE": 152.99, "R²": 0.73}
}

# Convertir en DataFrame
df_results = pd.DataFrame(results).T.reset_index()
df_results.rename(columns={"index": "Modèle"}, inplace=True)

# --- 📊 Visualisation ---
plt.figure(figsize=(12,5))

# RMSE
plt.subplot(1,2,1)
sns.barplot(data=df_results, x="Modèle", y="RMSE", palette="Blues_r")
plt.xticks(rotation=45, ha="right")
plt.title("Comparaison des modèles (RMSE)")
plt.ylabel("Erreur (plus bas = mieux)")

# R²
plt.subplot(1,2,2)
sns.barplot(data=df_results, x="Modèle", y="R²", palette="Greens_r")
plt.xticks(rotation=45, ha="right")
plt.title("Comparaison des modèles (R²)")
plt.ylabel("Variance expliquée (plus haut = mieux)")

plt.tight_layout()
plt.show()